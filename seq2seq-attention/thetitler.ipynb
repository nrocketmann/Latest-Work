{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.txt\n",
      "sport\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dictionary = {'articles':[],'titles':[],'subtitles':[]}\n",
    "for d in os.listdir('bbc'):\n",
    "    if d=='README.TXT' or d=='.DS_Store':\n",
    "        continue\n",
    "    for f in os.listdir('bbc/' + d):\n",
    "        if f=='.DS_Store':\n",
    "            continue\n",
    "        try:   \n",
    "            lines = open('bbc/' + d + '/' + f).readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f)\n",
    "            print(d)\n",
    "            continue\n",
    "        content = ' '.join(lines[3:]).replace('\\n',' ')\n",
    "        title = lines[0]\n",
    "        subtitle = lines[2]\n",
    "        dictionary['articles'].append(content)\n",
    "        dictionary['titles'].append(title)\n",
    "        dictionary['subtitles'].append(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize as wpt\n",
    "\n",
    "def build_dictionary(docs):\n",
    "    uniques = set()\n",
    "    dictionary = {}\n",
    "    reverse_dictionary={}\n",
    "    for doc in docs:\n",
    "        toks = wpt(doc)\n",
    "        for tok in toks:\n",
    "            uniques.add(tok)\n",
    "    for i, tok in enumerate(list(uniques)):\n",
    "        dictionary[tok] = i\n",
    "        reverse_dictionary[i] = tok\n",
    "    return dictionary,reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,rwords = build_dictionary(dictionary['titles']+dictionary['subtitles']+dictionary['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {'articles':[],'titles':[],'subtitles':[]}\n",
    "def numerify(text):\n",
    "    toks = wpt(text)\n",
    "    return np.array(list(map(lambda x: words[x],toks)))\n",
    "\n",
    "for article,title,subtitle in zip(dictionary['articles'],dictionary['titles'],dictionary['subtitles']):\n",
    "    index_dict['articles'].append(numerify(article))\n",
    "    index_dict['titles'].append(numerify(title))\n",
    "    index_dict['subtitles'].append(numerify(subtitle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['<STOP>'] = len(words)\n",
    "rwords[max(rwords.keys())+1] = '<STOP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should fill in the excess characters in the titles with stops (11 is max length of a title)\n",
    "for i in range(len(index_dict['titles'])):\n",
    "    to_add = []\n",
    "    title = index_dict['titles'][i]\n",
    "    for k in range(11-title.shape[0]):\n",
    "        to_add.append(words['<STOP>'])\n",
    "    together = np.concatenate([title,np.array(to_add)],axis=0)\n",
    "    index_dict['titles'][i] = together\n",
    "assert list(set([x.shape[0] for x in index_dict['titles']])) == [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7335 10299  8704  3512 17565 33757 33757 33757 33757 33757 33757]\n"
     ]
    }
   ],
   "source": [
    "print(index_dict['titles'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(index_dict,open('index_dict.pkl','wb'))\n",
    "pickle.dump(words,open('dictionary.pkl','wb'))\n",
    "pickle.dump(rwords,open('reverse_dictionary.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = pickle.load(open('index_dict.pkl','rb'))\n",
    "words = pickle.load(open('dictionary.pkl','rb'))\n",
    "rwords = pickle.load(open('reverse_dictionary.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,batch_sz,enc_units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    def call(self,inp,init_state):\n",
    "        x = self.embedding(inp)\n",
    "        output,state = self.gru(x,initial_state=init_state)\n",
    "        return output, state\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return tf.zeros(shape=[self.batch_sz,self.enc_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,window_size,enc_units):\n",
    "        super(Attention,self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.score_W = tf.keras.layers.Dense(self.enc_units) #must be the same size as hs\n",
    "        \n",
    "        self.window_W1 = tf.keras.layers.Dense(10,activation='tanh')\n",
    "        self.window_W2 = tf.keras.layers.Dense(1,activation='softmax')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def score(self,ht,hs):\n",
    "        return tf.matmul(hs,tf.transpose(self.score_W(ht))) #1xenc_units times enc_unitsx1 (I think...)\n",
    "        \n",
    "    def call(self,source,ht): #source should be [batch, num_words, dims]\n",
    "        \n",
    "        S = tf.cast(source.shape[1],tf.float32)\n",
    "        pt = self.window_W2(self.window_W1(ht))*S\n",
    "        rounded = int(np.round(pt))\n",
    "        unstacked_source = tf.unstack(source,axis=1) #giving us all the \"hs\" we need\n",
    "        \n",
    "        gaussians = []\n",
    "        alignment_partials = []\n",
    "        \n",
    "        lower_bound = rounded-self.window_size-1\n",
    "        if lower_bound<0:\n",
    "            lower_bound = 0\n",
    "        upper_bound = rounded+self.window_size+1\n",
    "        if upper_bound>len(unstacked_source):\n",
    "            upper_bound = len(unstacked_source)\n",
    "        \n",
    "        for i in range(lower_bound,upper_bound):\n",
    "            gaussian = tf.math.exp(-tf.math.square(pt-i)/2/tf.math.square(self.window_size/2))\n",
    "            alignment_partial = tf.math.exp(self.score(ht,unstacked_source[i]))\n",
    "            alignment_partials.append(alignment_partial)\n",
    "            gaussians.append(gaussian)\n",
    "        \n",
    "        alignment_sum = tf.reduce_sum(alignment_partials)\n",
    "        attention_vecs = []\n",
    "        for partial,gauss,hs_ in zip(alignment_partials,gaussians,unstacked_source[rounded-self.window_size:rounded+self.window_size+1]):\n",
    "            attention_vecs.append(partial/alignment_sum*gauss*hs_)\n",
    "        return tf.reduce_sum(attention_vecs,axis=0)*(1/(2*self.window_size+1)) #this should have size enc_units\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,dec_units,batch_sz,enc_units):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences = True,\n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(10,enc_units)\n",
    "    \n",
    "    def call(self,x,hidden,enc_output):\n",
    "        context = self.attention(enc_output,hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "        output,state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2])) #do this b/c we're doing just 1 timestep\n",
    "        return self.fc(output),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_dim = 64\n",
    "enc_units = 256\n",
    "dec_units = 256\n",
    "vocab_size = len(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 426, 256)\n",
      "(1, 256)\n",
      "(1, 33758)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size,embedding_dim,batch_size,enc_units)\n",
    "decoder = Decoder(vocab_size,embedding_dim,dec_units,batch_size,enc_units)\n",
    "\n",
    "sample = index_dict['articles'][0]\n",
    "test_enc = encoder(np.reshape(sample,[1,sample.shape[0]]),encoder.init_hidden())\n",
    "print(test_enc[0].shape)\n",
    "print(test_enc[1].shape)\n",
    "\n",
    "sample_dec = index_dict['titles'][0]\n",
    "test_dec = decoder(np.reshape(sample_dec[0],[1,1]),test_enc[1],test_enc[0])\n",
    "print(test_dec[0].shape)\n",
    "print(test_dec[1].shape)\n",
    "#woot!!! Everything works here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batchx,batchy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoded,state = encoder(batchx,encoder.init_hidden())\n",
    "        #losses = tf.zeros(shape=batchx.shape[0]-1)\n",
    "        for i in range(batchx.shape[0]-1):\n",
    "            pred,state = decoder(np.reshape(batchx[i],[1,1]),state,encoded)\n",
    "            loss = loss_function(batchy[i+1],pred)\n",
    "            grad = tape.gradient(loss_sum, encoder.trainable_variables+decoder.trainable_variables)\n",
    "            print(grad)\n",
    "#         loss_sum = tf.reduce_sum(losses)\n",
    "#         batch_loss = loss_sum/10\n",
    "#         variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "#         gradients = tape.gradient(loss_sum, variables)\n",
    "#         print(gradients)\n",
    "#         optimizer.apply_gradients(zip(gradients, variables))\n",
    "#     return batch_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(index_dict['titles'])):\n",
    "    batchx = index_dict['articles'][n]\n",
    "    bx = np.reshape(batchx,[1,batchx.shape[0]])\n",
    "    l=train_step(bx,index_dict['titles'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "\n",
    "losser = loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "trainable_vars = emb_layer.trainable_variables + enc_layer.trainable_variables + dec_gru.trainable_variables + out_W.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel,self).__init__()\n",
    "        #now for the fast, easy version without attention!\n",
    "\n",
    "        self.seq_len=11\n",
    "        self.emb_layer = layers.Embedding(len(words),100)\n",
    "        self.enc_layer = layers.GRU(return_state=True,units=256)\n",
    "        self.transition = layers.Dense(100)\n",
    "        self.dec_gru = layers.GRU(return_state=True,return_sequences=True,units=256)\n",
    "        self.to_embed = layers.Dense(100)\n",
    "        self.out_W = layers.Dense(len(words),activation='softmax')\n",
    "\n",
    "    def call(self,article):\n",
    "        embedded = self.emb_layer(article)\n",
    "        encoded,state = self.enc_layer(embedded)\n",
    "        transition = tf.reshape(self.transition(encoded),[1,1,100])\n",
    "        out = tf.TensorArray(tf.float32,size=11)\n",
    "        for i in range(11):\n",
    "            temp,state = self.dec_gru(transition,initial_state = state)\n",
    "            transition = tf.reshape(self.to_embed(temp),[1,1,100])\n",
    "            out.write(i,tf.squeeze(self.out_W(transition)))\n",
    "        return out.stack()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 33758)\n"
     ]
    }
   ],
   "source": [
    "art1 = index_dict['articles'][0]\n",
    "# art2 = index_dict['articles'][1]\n",
    "art1 = np.reshape(art1,[1,art1.shape[0]])\n",
    "# art2 = np.reshape(art2,[1,art2.shape[0]])\n",
    "simp_model = SimpleModel()\n",
    "print(simp_model(art1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "losser = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_batch_simple(articlebatch,titlebatch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = simp_model(articlebatch)\n",
    "        loss = losser(titlebatch,pred)\n",
    "        grad = tape.gradient(loss,simp_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,simp_model.trainable_variables))\n",
    "        #print(np.sum([tf.reduce_sum(g) for g in grad]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016065120697021484\n",
      "3.8235268592834473\n",
      "tf.Tensor(10.426876, shape=(), dtype=float32)\n",
      "0.0008268356323242188\n",
      "5.693143129348755\n",
      "tf.Tensor(10.425727, shape=(), dtype=float32)\n",
      "9.703636169433594e-05\n",
      "3.6416802406311035\n",
      "tf.Tensor(10.421902, shape=(), dtype=float32)\n",
      "9.202957153320312e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-b175f0d80f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mybatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'titles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-66fd2a6fd8b6>\u001b[0m in \u001b[0;36mtrain_batch_simple\u001b[0;34m(articlebatch, titlebatch)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticlebatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitlebatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(np.sum([tf.reduce_sum(g) for g in grad]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_StridedSliceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;31m# We could choose any of {begin|end|strides}.dtype since they are required to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;31m# be the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   return array_ops.strided_slice_grad(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m   \"\"\"\n\u001b[0;32m--> 330\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m   6517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6518\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6519\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_symbolic_input_in_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6520\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6521\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_eager_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for k in range(10000):\n",
    "    t = time.time()\n",
    "    ind = np.random.randint(0,len(index_dict['titles']))\n",
    "    xbatch = index_dict['articles'][ind]\n",
    "    xbatch = np.reshape(xbatch,[1,xbatch.shape[0]])\n",
    "    ybatch = index_dict['titles'][ind]\n",
    "    print(time.time()-t)\n",
    "    l = train_batch_simple(xbatch,ybatch)\n",
    "    print(time.time()-t)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     multiple                  3375800   \n",
      "_________________________________________________________________\n",
      "gru_27 (GRU)                 multiple                  274176    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             multiple                  25700     \n",
      "_________________________________________________________________\n",
      "gru_28 (GRU)                 multiple                  274176    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             multiple                  25700     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             multiple                  3409558   \n",
      "=================================================================\n",
      "Total params: 7,385,110\n",
      "Trainable params: 7,385,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[ 2455.,  2377., 28177., ..., 33757., 33757., 33757.],\n       [ 3358., 29353., 20227., ..., 33757., 33757., 33757.],\n       [14635., 24113., 15114., ..., 33757., 33757., 33757.],\n       ...,\n       [15250., 28947., 29790., ..., 33757., 33757., 33757.],\n       [21014., 26166., 21597., ..., 33757., 33757., 33757.],\n       [ 6811., 17129.,  2377., ..., 33757., 33757., 33757.]]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-f43ccb6791ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'titles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2671\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    303\u001b[0m       raise ValueError(\n\u001b[1;32m    304\u001b[0m           \u001b[0;34m'Error when checking model '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m           'expected no data, but got:', data)\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[ 2455.,  2377., 28177., ..., 33757., 33757., 33757.],\n       [ 3358., 29353., 20227., ..., 33757., 33757., 33757.],\n       [14635., 24113., 15114., ..., 33757., 33757., 33757.],\n       ...,\n       [15250., 28947., 29790., ..., 33757., 33757., 33757.],\n       [21014., 26166., 21597., ..., 33757., 33757., 33757.],\n       [ 6811., 17129.,  2377., ..., 33757., 33757., 33757.]]))"
     ]
    }
   ],
   "source": [
    "simp_model.compile(loss=losser,optimizer=optimizer)\n",
    "simp_model.fit(np.array(index_dict['articles']),np.array(index_dict['titles']),batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's just try this all with keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inp = tf.keras.Input(shape=[None],dtype=tf.int32)\n",
    "emb_enc = layers.Embedding(len(words),100)(inp)\n",
    "enc,state = layers.GRU(return_state=True,return_sequences=True,units=256)(emb_enc)\n",
    "\n",
    "dec_gru_ = layers.GRU(return_sequences=True,return_state=True,units=256)\n",
    "out=tf.TensorArray(dtype=tf.float32,size=11)\n",
    "word_final_ = layers.Dense(len(words),activation='softmax')\n",
    "for i in range(int(out.size())):\n",
    "    enc,state = dec_gru_(enc,initial_state=state)\n",
    "    out.write(i,word_final_(enc))\n",
    "final = out.stack()\n",
    "\n",
    "model_comp = tf.keras.models.Model(inputs=inp,outputs=final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.compile(loss=losser,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n",
      "2224\n"
     ]
    }
   ],
   "source": [
    "print(len(index_dict['articles']))\n",
    "print(len(index_dict['titles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.ragged.constant(index_dict['articles'])\n",
    "Y=tf.ragged.constant(index_dict['titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Dimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ef17f9d526ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2686\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m         \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    466\u001b[0m       ])\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m   \u001b[0mset_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_of_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m   \u001b[0mset_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_of_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m   \u001b[0mset_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_of_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mset_of_lengths\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    462\u001b[0m       return set([\n\u001b[1;32m    463\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m       ])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Dimension'"
     ]
    }
   ],
   "source": [
    "model_comp.fit(X,Y,batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
